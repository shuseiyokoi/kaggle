{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2899777b-1de4-43f7-80cc-72d24d23a141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 82\n",
      "[LightGBM] [Info] Number of data points in the train set: 5872, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 5542.194142\n",
      "MAPE Results for Each Model:\n",
      "                            Model     MAPE\n",
      "0                    XGBRegressor  0.03999\n",
      "1   HistGradientBoostingRegressor  0.04350\n",
      "2                   LGBMRegressor  0.04351\n",
      "3           RandomForestRegressor  0.04492\n",
      "5             ExtraTreesRegressor  0.04659\n",
      "4                BaggingRegressor  0.04733\n",
      "7           DecisionTreeRegressor  0.05810\n",
      "8              ExtraTreeRegressor  0.06146\n",
      "6       GradientBoostingRegressor  0.07577\n",
      "10            KNeighborsRegressor  0.20448\n",
      "9               AdaBoostRegressor  0.23689\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor, BaggingRegressor, ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "\n",
    "# Ignore Warnings\n",
    "# To avoid warnings cluttering the output, we disable them here\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Data Loading\n",
    "# Load training and testing datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Save Test IDs\n",
    "# Store the 'id' column from the test set for later use in submission\n",
    "test_id = test_df['id']\n",
    "\n",
    "# Data Preprocessing\n",
    "# Convert 'date' column to datetime format in the training data\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "# Fill missing values in 'holiday_name' column with 'None'\n",
    "train_df['holiday_name'].fillna('None', inplace=True)\n",
    "test_df['holiday_name'].fillna('None', inplace=True)\n",
    "\n",
    "# Select base features from test data (excluding 'id')\n",
    "base_features = test_df.drop(columns=['id']).columns\n",
    "\n",
    "# Combine training and testing datasets for uniform preprocessing\n",
    "train_df = pd.concat([train_df[base_features], train_df['orders']], axis=1)\n",
    "test_df = test_df[base_features]\n",
    "\n",
    "# Merge Data for Preprocessing\n",
    "# Combine both datasets to apply transformations consistently\n",
    "all_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n",
    "\n",
    "# Feature Engineering - Date\n",
    "# Extract year, month, day, and day of the week from the 'date' column\n",
    "date_col = ['date']\n",
    "for _col in date_col:\n",
    "    date_col = pd.to_datetime(all_df[_col], errors='coerce')\n",
    "    all_df[_col + \"_year\"] = date_col.dt.year.fillna(-1)\n",
    "    all_df[_col + \"_month\"] = date_col.dt.month.fillna(-1)\n",
    "    all_df[_col + \"_day\"] = date_col.dt.day.fillna(-1)\n",
    "    all_df[_col + \"_day_of_week\"] = date_col.dt.dayofweek.fillna(-1)\n",
    "    all_df.drop(_col, axis=1, inplace=True)\n",
    "\n",
    "# Fill missing 'holiday_name' values\n",
    "all_df['holiday_name'].fillna('None', inplace=True)\n",
    "\n",
    "# Encoding Categorical Features\n",
    "# One-Hot Encoding for 'holiday_name' column\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "holiday_encoded = enc.fit_transform(all_df[['holiday_name']])\n",
    "encoded_df = pd.DataFrame(holiday_encoded, columns=enc.get_feature_names_out(['holiday_name']))\n",
    "all_df = pd.concat([all_df, encoded_df], axis=1)\n",
    "all_df = all_df.drop('holiday_name', axis=1)\n",
    "\n",
    "# Label Encoding for 'warehouse' column\n",
    "le = preprocessing.LabelEncoder()\n",
    "all_df['warehouse'] = le.fit_transform(all_df['warehouse'])\n",
    "\n",
    "# Split Data Back into Train and Test\n",
    "# Separate the combined dataset back into training and test sets\n",
    "train_df_le = all_df[~all_df['orders'].isnull()]\n",
    "test_df_le = all_df[all_df['orders'].isnull()]\n",
    "\n",
    "# Features and Target Separation\n",
    "# Separate features (X) and target variable (y)\n",
    "X = train_df_le.drop(columns=['orders'])\n",
    "y = train_df_le['orders']\n",
    "\n",
    "# Train-Test Split\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=777)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'XGBRegressor': XGBRegressor(),\n",
    "    'HistGradientBoostingRegressor': HistGradientBoostingRegressor(),\n",
    "    'LGBMRegressor': LGBMRegressor(),\n",
    "    'RandomForestRegressor': RandomForestRegressor(),\n",
    "    'BaggingRegressor': BaggingRegressor(),\n",
    "    'ExtraTreesRegressor': ExtraTreesRegressor(),\n",
    "    'GradientBoostingRegressor': GradientBoostingRegressor(),\n",
    "    'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "    'ExtraTreeRegressor': ExtraTreeRegressor(),\n",
    "    'AdaBoostRegressor': AdaBoostRegressor(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor()\n",
    "}\n",
    "\n",
    "# Initialize list to hold MAPE results\n",
    "mape_results = []\n",
    "\n",
    "# Train, predict, and calculate MAPE for each model\n",
    "for model_name, model in models.items():\n",
    "    # Create a pipeline with standard scaling\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on validation set\n",
    "    pred_val = pipeline.predict(X_val)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = mean_absolute_percentage_error(y_val, pred_val)\n",
    "    \n",
    "    # Append results\n",
    "    mape_results.append({'Model': model_name, 'MAPE': mape})\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "mape_df = pd.DataFrame(mape_results)\n",
    "\n",
    "# Format MAPE to 5 decimal places\n",
    "mape_df['MAPE'] = mape_df['MAPE'].apply(lambda x: f\"{x:.5f}\")\n",
    "\n",
    "# Sort by MAPE\n",
    "mape_df = mape_df.sort_values(by='MAPE')\n",
    "\n",
    "# Print MAPE results\n",
    "print(\"MAPE Results for Each Model:\")\n",
    "print(mape_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f870dc4-89e4-4dc0-8508-6b4eafb68e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAPE for XGBRegressor: 0.03928\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the XGBRegressor model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "\n",
    "\n",
    "# Print MAPE\n",
    "print(f\"\\nMAPE for XGBRegressor: {mape:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8cedfe2-0ccd-47bf-a319-42af702a86b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "                                              Feature  Importance\n",
      "0                                           warehouse    0.622150\n",
      "30                         holiday_name_New Years Day    0.092631\n",
      "5                                           date_year    0.072311\n",
      "12                         holiday_name_Christmas Eve    0.047119\n",
      "6                                          date_month    0.032187\n",
      "1                                             holiday    0.025168\n",
      "2                                        shops_closed    0.024899\n",
      "8                                    date_day_of_week    0.021119\n",
      "16                   holiday_name_Den ceske statnosti    0.015761\n",
      "17                        holiday_name_Den osvobozeni    0.013576\n",
      "7                                            date_day    0.005830\n",
      "23                               holiday_name_Jan Hus    0.004591\n",
      "19                         holiday_name_Easter Monday    0.004160\n",
      "4                                     school_holidays    0.003603\n",
      "3                              winter_school_holidays    0.003430\n",
      "20                           holiday_name_Good Friday    0.002608\n",
      "13                      holiday_name_Cyrila a Metodej    0.002574\n",
      "31                                  holiday_name_None    0.001775\n",
      "22              holiday_name_International womens day    0.001161\n",
      "10                     holiday_name_2nd Christmas Day    0.000635\n",
      "14                 holiday_name_Day of National Unity    0.000502\n",
      "15      holiday_name_Den boje za svobodu a demokracii    0.000412\n",
      "24                            holiday_name_Labour Day    0.000412\n",
      "11               holiday_name_All Saints' Day Holiday    0.000350\n",
      "18  holiday_name_Den vzniku samostatneho ceskoslov...    0.000305\n",
      "28          holiday_name_Memorial Day of the Republic    0.000262\n",
      "27  holiday_name_Memorial Day for the Victims of t...    0.000217\n",
      "33                       holiday_name_Reformation Day    0.000107\n",
      "21               holiday_name_Independent Hungary Day    0.000083\n",
      "26  holiday_name_Memorial Day for the Victims of t...    0.000062\n",
      "25  holiday_name_Memorial Day for the Martyrs of Arad    0.000000\n",
      "29                  holiday_name_National Defense Day    0.000000\n",
      "9   holiday_name_1848 Revolution Memorial Day (Ext...    0.000000\n",
      "32            holiday_name_Peace Festival in Augsburg    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Print feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "features = X.columns if hasattr(X, 'columns') else np.arange(X.shape[1])\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82529314-ea61-43e0-bf95-b21a4f540e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAPE for Optimized XGBRegressor: 0.03730\n"
     ]
    }
   ],
   "source": [
    "# Define the best hyperparameters\n",
    "best_params = {\n",
    "    'colsample_bytree': 0.9,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 9,\n",
    "    'min_child_weight': 5,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.9\n",
    "}\n",
    "\n",
    "# Initialize and train the XGBRegressor model with best hyperparameters\n",
    "model = XGBRegressor(**best_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(y_val, y_pred)\n",
    "\n",
    "# Print MAPE\n",
    "print(f\"\\nMAPE for Optimized XGBRegressor: {mape:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b883f23e-d4b8-4335-8d62-1d34c0c07774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id        orders\n",
      "0      Prague_1_2024-03-16  10325.419922\n",
      "1      Prague_1_2024-03-17  10102.911133\n",
      "2      Prague_1_2024-03-18   9737.808594\n",
      "3      Prague_1_2024-03-19   9722.055664\n",
      "4      Prague_1_2024-03-20   9534.790039\n",
      "..                     ...           ...\n",
      "392  Budapest_1_2024-05-11   7141.117676\n",
      "393  Budapest_1_2024-05-12   6339.903809\n",
      "394  Budapest_1_2024-05-13   6513.333008\n",
      "395  Budapest_1_2024-05-14   6788.502930\n",
      "396  Budapest_1_2024-05-15   6627.637207\n",
      "\n",
      "[397 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "pred_test = model.predict(test_df_le.drop(columns=['orders']))\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_id,\n",
    "    'orders': pred_test\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission.to_csv('prediction_python_xgboost.csv', index=False)\n",
    "\n",
    "# Print the submission DataFrame\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4163706e-6c5f-4ae3-8493-7a3c17898b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
